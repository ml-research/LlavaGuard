{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfss/mlde/workspaces/mlde_wsp_KIServiceCenter/lhelff/envs/sglang/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-12 15:53:22,246\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sglang version: 0.4.1.post4\n",
      "Evaluating model: AIML-TUDA/LlavaGuard-v1.2-0.5B-OV\n",
      "Tokenizer not found for model LlavaGuard-v1.2-0.5B-OV using default tokenizer.\n",
      "Model LlavaGuard-v1.2-0.5B-OV prepared for sglang! Ready to evaluate.\n",
      "Launching server at GPU 0 with model: AIML-TUDA/LlavaGuard-v1.2-0.5B-OV and base_url: http://127.0.0.1:14668 and other_args: ['--chat-template', 'chatml-llava', '--tp', '1'], Server Engine: <class 'llavaguard.server.sglang.SGLangServer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:53:32] server_args=ServerArgs(model_path='AIML-TUDA/LlavaGuard-v1.2-0.5B-OV', tokenizer_path='AIML-TUDA/LlavaGuard-v1.2-0.5B-OV', tokenizer_mode='auto', load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, context_length=None, device='cuda', served_model_name='AIML-TUDA/LlavaGuard-v1.2-0.5B-OV', chat_template='chatml-llava', is_embedding=False, revision=None, skip_tokenizer_init=False, return_token_ids=False, host='127.0.0.1', port=14668, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='lpm', schedule_conservativeness=1.0, cpu_offload_gb=0, prefill_only_one_req=False, tp_size=1, stream_interval=1, random_seed=599105703, constrained_json_whitespace_pattern=None, watchdog_timeout=300, download_dir=None, base_gpu_id=0, log_level='info', log_level_http=None, log_requests=False, show_time_cost=False, enable_metrics=False, decode_log_interval=40, api_key='sk-123456', file_storage_pth='SGLang_storage', enable_cache_report=False, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', lora_paths=None, max_loras_per_batch=8, attention_backend='flashinfer', sampling_backend='flashinfer', grammar_backend='outlines', speculative_draft_model_path=None, speculative_algorithm=None, speculative_num_steps=5, speculative_num_draft_tokens=64, speculative_eagle_topk=8, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_jump_forward=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_mla=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_ep_moe=False, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=160, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False)\n",
      "[2025-02-12 15:53:33] Use chat template for the OpenAI-compatible API server: chatml-llava\n",
      "[2025-02-12 15:53:43 TP0] Overlap scheduler is disabled for multimodal models.\n",
      "[2025-02-12 15:53:43 TP0] Init torch distributed begin.\n",
      "[rank0]:[W212 15:53:43.341932714 ProcessGroupGloo.cpp:715] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[2025-02-12 15:53:43 TP0] Load weight begin. avail mem=78.43 GB\n",
      "[2025-02-12 15:53:43 TP0] Ignore import error when loading sglang.srt.models.dbrx. No module named 'sgl_kernel'\n",
      "[2025-02-12 15:53:43 TP0] Ignore import error when loading sglang.srt.models.deepseek. No module named 'sgl_kernel'\n",
      "[2025-02-12 15:53:43 TP0] Ignore import error when loading sglang.srt.models.deepseek_v2. No module named 'sgl_kernel'\n",
      "[2025-02-12 15:53:43 TP0] Ignore import error when loading sglang.srt.models.grok. No module named 'sgl_kernel'\n",
      "[2025-02-12 15:53:44 TP0] Ignore import error when loading sglang.srt.models.mixtral. No module named 'sgl_kernel'\n",
      "[2025-02-12 15:53:44 TP0] Ignore import error when loading sglang.srt.models.olmoe. No module named 'sgl_kernel'\n",
      "[2025-02-12 15:53:44 TP0] Ignore import error when loading sglang.srt.models.qwen2_moe. No module named 'sgl_kernel'\n",
      "[2025-02-12 15:53:44 TP0] Ignore import error when loading sglang.srt.models.xverse_moe. No module named 'sgl_kernel'\n",
      "[2025-02-12 15:53:48 TP0] Using model weights format ['*.safetensors']\n",
      "[2025-02-12 15:53:48 TP0] No model.safetensors.index.json found in remote.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.89s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "[2025-02-12 15:53:50 TP0] Load weight end. type=LlavaQwenForCausalLM, dtype=torch.float16, avail mem=76.54 GB\n",
      "[2025-02-12 15:53:50 TP0] lm_eval is not installed, GPTQ may not be usable\n",
      "[2025-02-12 15:53:51 TP0] Memory pool end. avail mem=12.29 GB\n",
      "[2025-02-12 15:53:51 TP0] Capture cuda graph begin. This can take up to several minutes.\n",
      "100%|██████████| 23/23 [00:07<00:00,  2.92it/s]\n",
      "[2025-02-12 15:53:58 TP0] Capture cuda graph end. Time elapsed: 7.88 s\n",
      "[2025-02-12 15:54:01 TP0] max_total_num_tokens=5564077, max_prefill_tokens=16384, max_running_requests=4097, context_len=32768\n",
      "[2025-02-12 15:54:01] INFO:     Started server process [86270]\n",
      "[2025-02-12 15:54:01] INFO:     Waiting for application startup.\n",
      "[2025-02-12 15:54:01] INFO:     Application startup complete.\n",
      "[2025-02-12 15:54:01] INFO:     Uvicorn running on http://127.0.0.1:14668 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:54:02] INFO:     127.0.0.1:36984 - \"GET /get_model_info HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:54:02 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:54:03] INFO:     127.0.0.1:36998 - \"GET /health_generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:54:03 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 1, #queue-req: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:54:03] INFO:     127.0.0.1:36996 - \"POST /generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:54:03] The server is fired up and ready to roll!\n"
     ]
    }
   ],
   "source": [
    "from llavaguard.server.server import set_up_server\n",
    "available_models = ['AIML-TUDA/LlavaGuard-v1.2-0.5B-OV', 'AIML-TUDA/LlavaGuard-v1.2-7B-OV']\n",
    "selected_model = available_models[0]\n",
    "server = set_up_server(engine='sglang', model_dir=selected_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed71a06e6ece56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:54:43 TP0] Prefill batch. #new-seq: 1, #new-token: 8192, #cached-token: 14, cache hit rate: 0.08%, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-02-12 15:54:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1610, #cached-token: 0, cache hit rate: 0.08%, token usage: 0.00, #running-req: 0, #queue-req: 1\n",
      "[2025-02-12 15:54:43 TP0] Decode batch. #running-req: 1, #token: 9856, token usage: 0.00, gen throughput (token/s): 1.08, #queue-req: 0\n",
      "[2025-02-12 15:54:43 TP0] Decode batch. #running-req: 1, #token: 9896, token usage: 0.00, gen throughput (token/s): 383.27, #queue-req: 0\n",
      "[2025-02-12 15:54:43 TP0] Decode batch. #running-req: 1, #token: 9936, token usage: 0.00, gen throughput (token/s): 383.87, #queue-req: 0\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-12 15:54:43] INFO:     127.0.0.1:55190 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n    \"rating\": \"Safe\",\\n    \"category\": \"NA: None applying\",\\n    \"rationale\": \"The image provided by the user is a photograph of a street scene with a stop sign, a car, and a building with a traditional architectural style. There are no people visible in the image, and it does not contain any content that would fall under any of the policy categories listed. The image is a simple representation of an urban environment and does not contain any elements that would be considered hate speech, violence, sexual content, nudity, criminal planning, weapons, substance abuse, self-harm, animal cruelty, or disasters or emergencies. Therefore, the content is considered safe and does not violate any of the policy guidelines.\"\\n}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from llavaguard.taxonomy.policy import get_assessment_and_system_prompt, get_default_policy\n",
    "prompt = get_default_policy('v24')\n",
    "image = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "inputs = [\n",
    "    {\n",
    "       'image': image,\n",
    "       'prompt': prompt\n",
    "    }\n",
    "]\n",
    "out = server.request(inputs=inputs)\n",
    "out[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
