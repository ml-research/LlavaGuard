{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f208ef9879a02a81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:41:46.218167949Z",
     "start_time": "2024-02-04T15:41:46.106429702Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import os\n",
    "# os.environ['HF_HOME'] = '/media/dong/storrage-01/huggingface'\n",
    "print(os.getenv('HF_HOME'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614d1c75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:41:47.788016232Z",
     "start_time": "2024-02-04T15:41:47.693179167Z"
    }
   },
   "source": [
    "import argparse\n",
    "import torch\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import TextStreamer, set_seed"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa025594-e5eb-4abf-a555-68f262626a8b",
   "metadata": {},
   "source": [
    "# own\n",
    "categories_own_reduction_of_smid = \", \".join([\n",
    "\"Drugs\",\n",
    "\"Harm\",\n",
    "\"Cruelty\",\n",
    "\"Sexual\"\n",
    "\"Exploitation\",\n",
    "\"Inequality\",\n",
    "\"Injustice\",\n",
    "\"Degradation\",\n",
    "\"Unfairness\",\n",
    "\"Immoral actions\",\n",
    "#\"None\"\n",
    "])\n",
    "\n",
    "# chatgpt based on first run\n",
    "categories_firstrun_chatgpt = \", \".join([ \n",
    "    \"Violence and Harm\",\n",
    "    \"Inappropriate or Offensive Behavior\",\n",
    "    \"Sexual Content and Behavior\",\n",
    "    \"Drug Use and Abuse\",\n",
    "    \"Religious and Cultural Sensitivities\",\n",
    "    \"Social and Political Issues\",\n",
    "    \"Disasters and Emergencies\",\n",
    "    \"Graphic and Disturbing Content\" ,\n",
    "    \"Inappropriate Language and Gestures\",\n",
    "    \"None\"\n",
    "])\n",
    "\n",
    "# chatgpt based on first run\n",
    "pos_and_neg_categories_firstrun_chatgpt = \", \".join([ \n",
    "    \"Violence and Harm\",\n",
    "    \"Inappropriate or Offensive Behavior\",\n",
    "    \"Sexual Content and Behavior\",\n",
    "    \"Drug Use and Abuse\",\n",
    "    \"Religious and Cultural Sensitivities\",\n",
    "    \"Social and Political Issues\",\n",
    "    \"Disasters and Emergencies\",\n",
    "    \"Graphic and Disturbing Content\" ,\n",
    "    \"Inappropriate Language and Gestures\",\n",
    "    \"Peace and Care\",\n",
    "    \"Respectful and Appropriate Conduct\",\n",
    "    \"Platonic Relationships and Respect for Boundaries\",\n",
    "    \"Healthy Living and Sobriety\",\n",
    "    \"Religious Tolerance and Cultural Respect\",\n",
    "    \"Social Harmony and Political Cooperation\",\n",
    "    \"Wellness and Safety Assurance\",\n",
    "    \"Stability and Preparedness\",\n",
    "    \"Wholesome and Comforting Content\",\n",
    "    \"Polite Communication and Gestures\",\n",
    "])\n",
    "\n",
    "pos_and_neg_categories_firstrun_chatgpt_wNA = \", \".join([ \n",
    "    \"Violence and Harm\",\n",
    "    \"Inappropriate or Offensive Behavior\",\n",
    "    \"Sexual Content and Behavior\",\n",
    "    \"Drug Use and Abuse\",\n",
    "    \"Religious and Cultural Sensitivities\",\n",
    "    \"Social and Political Issues\",\n",
    "    \"Disasters and Emergencies\",\n",
    "    \"Graphic and Disturbing Content\" ,\n",
    "    \"Inappropriate Language and Gestures\",\n",
    "    \"Peace and Care\",\n",
    "    \"Respectful and Appropriate Conduct\",\n",
    "    \"Platonic Relationships and Respect for Boundaries\",\n",
    "    \"Healthy Living and Sobriety\",\n",
    "    \"Religious Tolerance and Cultural Respect\",\n",
    "    \"Social Harmony and Political Cooperation\",\n",
    "    \"Wellness and Safety Assurance\",\n",
    "    \"Stability and Preparedness\",\n",
    "    \"Wholesome and Comforting Content\",\n",
    "    \"Polite Communication and Gestures\",\n",
    "    \"N/A\"\n",
    "])\n",
    "\n",
    "current_prototype_test = \", \".join([ \n",
    "    '\"None\"',\n",
    "    '\"Peace or Care\"',\n",
    "    '\"Respectful or Appropriate Content\"',\n",
    "    '\"Polite Communication and Gestures\"',\n",
    "    '\"Violence and Harm\"',\n",
    "    '\"Explicit Sexual Content\"',\n",
    "    '\"Nudity\"',\n",
    "    '\"Suicide or Self Harm\"',\n",
    "    '\"Animal Cruelty\"',\n",
    "    '\"Guns or Illegal Weapons\"',\n",
    "    '\"Drug Use or Abuse\"',\n",
    "    '\"Illegal Activities\"',\n",
    "    '\"Obscene gestures\"', \n",
    "    '\"Disasters\"',\n",
    "    '\"Harassment\"',\n",
    "    '\"Humiliation\"',\n",
    "    '\"Discrimination\"',\n",
    "    #'\"Misinformation\"',\n",
    "    '\"Violation of Privacy\"',\n",
    "    '\"Terrorism or Extremist content\"',\n",
    "])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5646d01e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:41:49.786670996Z",
     "start_time": "2024-02-04T15:41:49.692860119Z"
    }
   },
   "source": [
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()\n",
    "    return image, image_tensor"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42c9fb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T15:41:51.915718977Z",
     "start_time": "2024-02-04T15:41:51.820548595Z"
    }
   },
   "source": [
    "def clear_conv(conv):\n",
    "    conv.messages = []\n",
    "    return conv"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98d4c0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T17:53:34.411180795Z",
     "start_time": "2024-02-04T17:53:33.832032978Z"
    }
   },
   "source": [
    "# model_path= \"liuhaotian/llava-v1.5-7b\"\n",
    "model_path= \"liuhaotian/llava-v1.5-13b\"\n",
    "# model_path= \"liuhaotian/llava-v1.6-34b\"\n",
    "# model_path= \"liuhaotian/llava-v1.6-mistral-7b\"\n",
    "model_base=None\n",
    "num_gpus=1\n",
    "# conv_mode=None\n",
    "conv_mode=None\n",
    "temperature=0.2\n",
    "max_new_tokens=512\n",
    "load_8bit=False\n",
    "load_4bit=True\n",
    "debug=False"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c313f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-04T17:53:35.177003602Z"
    },
    "is_executing": true
   },
   "source": [
    "disable_torch_init()\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "# tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name, load_8bit, load_4bit)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dd700f8-766e-4576-b0a6-1fc31a472a99",
   "metadata": {},
   "source": [
    "print('aa')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e37d533e-467a-42b5-94ab-eb56a5fe9674",
   "metadata": {},
   "source": [
    "if 'llama-2' in model_name.lower():\n",
    "    conv_mode = \"llava_llama_2\"\n",
    "elif \"llava-v1.6-34b\" in model_name.lower():\n",
    "    conv_mode = \"chatml_direct\"\n",
    "# elif \"llava-v1.6-mistral-7b\" in model_name.lower():\n",
    "#     conv_mode = \"llava_v1\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "    conv_mode = \"llava_v1\"\n",
    "elif \"mpt\" in model_name.lower():\n",
    "    conv_mode = \"mpt\"\n",
    "else:\n",
    "    conv_mode = \"llava_v0\"\n",
    "\n",
    "\n",
    "conv_ = conv_templates[conv_mode].copy()\n",
    "if \"mpt\" in model_name.lower():\n",
    "    roles = ('user', 'assistant')\n",
    "else:\n",
    "    roles = conv_.roles\n",
    "print(roles)\n",
    "print(conv_mode)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e645da8b-afcc-48b6-be4e-c0a9abb47300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:29:30.443530406Z",
     "start_time": "2024-02-04T13:29:30.397832295Z"
    }
   },
   "source": [
    "def run(image, conv, image_tensor, text=None, verbose=True):\n",
    "    if text is not None:\n",
    "        inp = text\n",
    "    else:    \n",
    "        try:\n",
    "            inp = input(f\"{roles[0]}: \")\n",
    "        except EOFError:\n",
    "            inp = \"\"\n",
    "        if not inp:\n",
    "            print(\"exit...\")\n",
    "    \n",
    "    if image is not None:\n",
    "        conv = clear_conv(conv)\n",
    "        # first message\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "        else:\n",
    "            inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "        conv.append_message(conv.roles[0], inp)\n",
    "        image = None\n",
    "    else:\n",
    "        # later messages\n",
    "        conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"{roles[1]}: \", end=\"\")\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    keywords = [stop_str]\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    attention_mask = input_ids.ne(0)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            images=image_tensor,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=1024,\n",
    "            streamer=streamer if verbose else None,\n",
    "            use_cache=True,\n",
    "            stopping_criteria=[stopping_criteria])\n",
    "    # outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
    "    outputs = tokenizer.decode(output_ids[0,:]).strip()\n",
    "    conv.messages[-1][-1] = outputs\n",
    "    return outputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2be131f8-cd8a-4b9a-b1e6-71a0b5f161df",
   "metadata": {},
   "source": [
    "prompt_template = {\n",
    "    'variant6_fixed_pos_and_neg_categories_chatgpt_1category_2score_LASTclassification_wBins': f\"Assess the inappropriateness of the content and classify if it is compliant to policies.\" + ' Use the following json template to answer: {\"content-category\": '+f'str<{pos_and_neg_categories_firstrun_chatgpt_wNA}>'+', \"score\": str<Explicitly Inappropriate,Marginally Inappropriate,Neutral,Generally Acceptable,Fully Appropriate>}, \"short-caption\": str, \"explanation\": str, \"final-assessment\": str<Compliant/Review Needed>',\n",
    "    'v7': f\"Assess the inappropriateness of the content and classify if it is compliant to policies.\" +\n",
    "' Use the following json template to answer: {\"content-category\": '+f'str<{current_prototype_test}>'+', \"short-caption\": str, \"explanation\": str, \"score\": str<\"Explicitly Inappropriate\", \"Highly Inappropriate\", \"Moderately Inappropriate\", \"Barely Acceptable\", \"Generally Acceptable\", \"Suitable & Respectful\", \"Positive & Engaging\", \"Exemplary & Inspiring\">}, \"final-assessment\": str<\"Review Needed\"/\"Compliant\">. A review is needed as soon as content is at least Marginally Inappropriate.',\n",
    "}\n",
    "print(prompt_template['v7'])\n",
    "\n",
    "json_keys = {\n",
    "    'variant6_fixed_pos_and_neg_categories_chatgpt_1category_2score_LASTclassification_wBins':\n",
    "    [\"content-category\", \"score\", \"short-caption\", \"explanation\", \"final-assessment\"],\n",
    "    'v7':\n",
    "    [\"content-category\", \"score\", \"short-caption\", \"explanation\", \"final-assessment\"],\n",
    "    'unlimited_categories_1category_2caption_3classification_wBins': \n",
    "    [\"inappropriateness-category\", \"score\", \"short-caption\", \"explanation\", \"final-assessment\"],\n",
    "    \n",
    "}\n",
    "\n",
    "def parse_json(j, prompt_key):\n",
    "    j = j.replace('</s>', '')\n",
    "    j = j.replace('<s>', '')\n",
    "    j = j.split('{')[1].split(\"}\")[0]\n",
    "    j = \"{\" + j + \"}\"\n",
    "    # remove all '' and \"\" from json\n",
    "    for e in json_keys[prompt_key]:\n",
    "        if e not in j:\n",
    "            raise ValueError(f'{e} missing in json. {j}')\n",
    "        if e+'\"' not in j:\n",
    "            j = j.replace(e+':', f'\"{e}\":')\n",
    "    return j\n",
    "    \n",
    "import warnings\n",
    "def print_json(out, selected_prompt_template):\n",
    "    try:\n",
    "        out = parse_json(out, selected_prompt_template)\n",
    "        json_object = json.loads(out)\n",
    "        for k in json_object.keys():\n",
    "            r = json_object[k]\n",
    "            if k == 'final-assessment':\n",
    "                r = r.replace('Review Needed', \"\\x1b[31mReview Needed\\x1b[0m\")\n",
    "                r = r.replace('Compliant', \"\\x1b[32mCompliant \\x1b[0m\")\n",
    "            elif k == 'score':\n",
    "                if 'Inappropriate' in r:\n",
    "                    r = f\"\\x1b[31m{r}\\x1b[0m\"\n",
    "                else:\n",
    "                    r = f\"\\x1b[32m{r}\\x1b[0m\"\n",
    "            print(k+':', r)\n",
    "    except Exception as e:\n",
    "        raise e"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "082a23ae-2e03-4627-9f6e-65e6e33becf1",
   "metadata": {},
   "source": [
    "def generate_single_response(image, image_tensor, text=None, verbose=True):\n",
    "    if text is not None:\n",
    "        inp = text\n",
    "    else:\n",
    "        inp = \"\"\n",
    "\n",
    "    if image is not None:\n",
    "        # first message\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "        else:\n",
    "            inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "\n",
    "    input_ids = tokenizer_image_token(inp, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "    attention_mask = input_ids.ne(0)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            images=image_tensor,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            streamer=streamer if verbose else None,\n",
    "            use_cache=True)\n",
    "    outputs = tokenizer.decode(output_ids[0,:]).strip()\n",
    "    return outputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a973c285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:29:36.588282572Z",
     "start_time": "2024-02-04T13:29:36.423380606Z"
    }
   },
   "source": [
    "print(prompt_template['v7'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d861231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-04T13:34:12.292871273Z",
     "start_time": "2024-02-04T13:33:50.815618765Z"
    }
   },
   "source": [
    "import glob\n",
    "import json\n",
    "set_seed(1)\n",
    "selected_prompt_template = \"v7\"\n",
    "image_paths = glob.glob('demo/*.png')\n",
    "print(f'Inference on {len(image_paths)} images')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d203ec10-4b5d-4234-98f6-2a947c5f62bf",
   "metadata": {},
   "source": [
    "image_, image_tensor_ = load_image('demo/warning.png')\n",
    "image_.resize((512,512)).show()\n",
    "for i in range(1):\n",
    "    out = run(image=image_, conv=conv_, image_tensor=image_tensor_, text=prompt_template[selected_prompt_template], verbose=True)\n",
    "    print_json(out, selected_prompt_template)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02c910bd-44ba-4866-bb6f-0e4b2baf0800",
   "metadata": {},
   "source": [
    "image_, image_tensor_ = load_image('demo/bomb.png')\n",
    "image_.resize((512,512)).show()\n",
    "for i in range(1):\n",
    "    out = run(image=image_, conv=conv_, image_tensor=image_tensor_, text=prompt_template[selected_prompt_template], verbose=True)\n",
    "    print_json(out, selected_prompt_template)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6ae4a45c-e407-4554-b0eb-576bab1b3297",
   "metadata": {},
   "source": [
    "image_, image_tensor_ = load_image('demo/asperin.png')\n",
    "image_.resize((512,512)).show()\n",
    "for i in range(1):\n",
    "    out = run(image=image_, conv=conv_, image_tensor=image_tensor_, text=prompt_template[selected_prompt_template], verbose=True)\n",
    "    print_json(out, selected_prompt_template)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "a7d48dfb-9d3c-44ce-9279-647495fda855",
   "metadata": {},
   "source": [
    "image_, image_tensor_ = load_image('demo/smoking.png')\n",
    "image_.resize((512,512)).show()\n",
    "for i in range(1):\n",
    "    out = run(image=image_, conv=conv_, image_tensor=image_tensor_, text=prompt_template[selected_prompt_template], verbose=True)\n",
    "    print_json(out, selected_prompt_template)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137bd60f-4f84-45d9-a394-89a4e74feb78",
   "metadata": {},
   "source": [
    "image_, image_tensor_ = load_image('demo/kid_weed.png')\n",
    "image_.resize((512,512)).show()\n",
    "for i in range(1):\n",
    "    out = run(image=image_, conv=conv_, image_tensor=image_tensor_, text=prompt_template[selected_prompt_template], verbose=True)\n",
    "    print_json(out, selected_prompt_template)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f57cc-9d47-43ad-a284-0e83a35c7d29",
   "metadata": {},
   "source": [
    "image_, image_tensor_ = load_image('demo/happy.png')\n",
    "image_.resize((512,512)).show()\n",
    "for i in range(1):\n",
    "    out = run(image=image_, conv=conv_, image_tensor=image_tensor_, text=prompt_template[selected_prompt_template], verbose=True)\n",
    "    print_json(out, selected_prompt_template)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a91a4-858f-47f1-b842-445950fce883",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
