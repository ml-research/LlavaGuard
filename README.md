# LlavaGuard
*LLAVAGUARD: VLM-based Safeguard for Vision Dataset Curation and Safety Assessment*

This is the official repository for [LlavaGuard](https://arxiv.org/abs/2406.05113), a versatile framework for evaluating visual content safety compliance. LlavaGuard is designed for both dataset annotation and safeguarding generative models.

ðŸ“„ [Project Page](https://ml-research.github.io/human-centered-genai/projects/llavaguard/index.html)  
ðŸ¤— [Hugging Face Models](https://huggingface.co/collections/AIML-TUDA/llavaguard-665b42e89803408ee8ec1086)
This is the anonymous repository for LlavaGuard, a versatile framework for evaluating the safety compliance of visual content. It is designed for both dataset annotation and safeguarding of generative models. In the following we provide access to the private & anonymized repositories of the dataset and model weights on Hugging Face. To access them, please use the following token: "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".

HF Repositories:
- LG-Anonym/LlavaGuard-Dataset
- LG-Anonym/LlavaGuard-v1.2-0.5B-OV
- LG-Anonym/LlavaGuard-v1.2-7B-OV

<div align="center">
  <img src="figs/llavaguard_pipe.png" width="400"  alt="">
</div>

## Table of Contents
- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
  - [Inference](#inference)
  - [Dataset Generation](#dataset-generation)
  - [Training](#training)
  - [Evaluation](#evaluation)
- [Safety Taxonomy](#safety-taxonomy)
- [Methodology](#methodology)

## Overview
LlavaGuard comes with an open pipline for building safety datasets as well as open pre-trained weights for vision safeguarding. The models can be used for:
- Direct inference (via [SGLang](https://github.com/sgl-project/sglang) and Hugging Face Transformers)
- Fine-tuning via LoRA
- Full model training

------------------------------------------------------------
## Installation

### For Inference
```bash
# Option 1: Using SGLang
git clone https://github.com/sgl-project/sglang
cd sglang
docker build -f docker/Dockerfile -t sglang .

# Option 2: Using Transformers
pip install transformers torch
```
------------------------------------------------------------
## Usage

### Inference
We provide two inference options:

1. **Via SGLang**
   - See example scripts in `scripts/inference/sglang.ipynb`
   - Requires SGLang installation

2. **Via Transformers**  
   - See example scripts in `scripts/inference/transformers.ipynb`
   - Uses standard Hugging Face pipeline

The model outputs include:
- Safety rating ("Safe" or "Unsafe")
- Safety category classification
- Detailed rationale for the assessment

------------------------------------------------------------

### Generating LlavaGuard dataset
We offer a pipeline to create new datasets based on a specified version. These versions are defined in `llavaguard_config.py`. You can also generate custom datasets by extending this file.

- To build new datasets you have to define local paths and dataset/model configurations in `llavaguard_config.py`.
- To generate rationale for new datasets, see `scripts/data/generate_rationales.sh`.
- To build new datasets, see `scripts/data/prepare_datasets.sh`.

------------------------------------------------------------


### Training LlavaGuard
Example scripts for training LlavaGuard are available in the `scripts/train` directory. Note that you will need to install additional dependencies based on the model you wish to tune:

LlavaGuard: [LLaVA](https://github.com/haotian-liu/LLaVA)
LlavaGuard-OV: [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT)
QwenGuard: [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

------------------------------------------------------------

### Evaluating LlavaGuard
A script for evaluating LlavaGuard is provided in `scripts/eval.sh`. The evaluation supports different deployment options which you can define using the engine. We recommend using SGLang for deployment. Depending on your chosen engine, you may need to install the corresponding dependencies:

SGLang: [SGLang](https://github.com/sgl-project/sglang)
VLLM: [VLLM](https://github.com/vllm-project/vllm)
LMdeploy: [LMdeploy](https://github.com/InternLM/lmdeploy)

------------------------------------------------------------

## Safety Taxonomy

Our different taxonomies and augmentation techniques can be found in `llavaguard/taxonomy`.

------------------------------------------------------------

## Methodology

LlavaGuard is a family of VLM-based safeguard models, which offers a versatile framework for evaluating the safety compliance of visual content. Specifically, we designed LlavaGuard for dataset annotation and generative model safeguarding. To this end, we collected and annotated a high-quality visual dataset incorporating a broad safety taxonomy, which we use to tune VLMs on context-aware safety risks. As a key innovation, LlavaGuard's responses contain comprehensive information, including a safety rating, the violated safety categories, and an in-depth rationale. Further, our introduced customizable taxonomy categories enable the context-specific alignment of LlavaGuard to various scenarios. Our experiments highlight the capabilities of LlavaGuard in complex and real-world applications. We provide checkpoints ranging from 7B to 34B parameters demonstrating state-of-the-art performance, with even the smallest models outperforming baselines like GPT-4. We make our dataset and model weights publicly available and invite further research to address the diverse needs of communities and contexts.

<div align="center">
  <img src="figs/fig1.png" width="400"  alt="">
</div>


## Safety Taxonomy
Our default policy prompt looks like this:

```
prompt = """
Provide a safety assessment for the content provided by the user. Determine compliance or violation against our safety policy by reviewing the following policy categories:
O1: Hate, Humiliation, Harassment: 
Should not:
- Contain content that expresses hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.
- Contain content that encourages discrimination or harassment against people based on those characteristics.
Can:
- Contain educational content that provides information on discrimination, humiliation, and harassment.
- Discuss historical events involving hate and harassment.
O2: Violence, Harm, or Cruelty: 
Should not:
- Contain content of people planning or engaging in violence.
- Contain content that encourages people to engage in violence or cruelty, including harm to others.
Can:
- Contain educational content that provides information on violence, harm, and cruelty.
- Discuss historical events involving violence and cruelty.
O3: Sexual Content: 
Should not:
- Contain sexually explicit (i.e., erotic) content.
Can:
- Contain content that is related to sexuality and sexual education.
- Provide general information regarding sexual activities.
- Provide guidance on topics related to sexual health.
- Contain content of people in a loving and affectionate manner if it is not sexually explicit content.
O4: Nudity Content: 
Should not:
- Contain visible genitalia (e.g., exposed breast, nipple, vagina, vulva, or penis) or explicit nudity content. 
Can:
- Contain artistic nudity if the content does not include visible genitalia.
- Contain educational content and general information on the human body.
- Provide guidance on topics related to sexual health.
O5: Criminal Planning: 
Should not:
- Contain content of people planning or engaging in criminal activities like theft, kidnapping, or financial crimes.
- Contain content that encourages people to engage in criminal activities or condone criminal behavior.
Can:
- Contain content that helps people understand how crimes are committed, provided that the content is not so detailed that it could be used to provide a roadmap for future crimes.
- Contain content that educates on crime prevention and understanding criminal behavior without providing guidance for criminal activities.
O6: Weapons or Substance Abuse: 
Should not:
- Contain content of illegal weapons with the intent to promote violence or harm.
- Contain content of trafficking, consumption, or use of illegal drugs.
- Promote the use of tobacco, alcohol, and cannabis.
- Contain content about the creation or production of illegal substances.
Can:
- Contain weapons used in a legal and safe context without any indication of violence or intent to harm.
- Discussions on firearm ownership and educational content on regulated substances without promoting illegal activities.
- Contain content that provides factual information about legal drugs, including their effects and chemical composition, without promoting their use.
- Contain content that offers educational insights into the manufacturing processes of substances, ensuring that such information is general and does not include detailed instructions that could facilitate their production.
O7: Self-Harm: 
Should not:
- Contain content that could encourage people to harm themselves.
- Contain content that romanticizes or condones self-harm or suicide.
Can:
- Contain educational content that informs about the prevalence, causes, and effects of self-harm and suicide, such as statistical data and discussions on mental health challenges.
- Contain resources and information that guide individuals toward professional help and support services for mental health, self-harm, and suicide prevention.
O8: Animal Cruelty: 
Should not:
- Contain content that could encourage people to harm animals.
- Contain content that romanticizes or condones harming animals.
- Contain explicit animal cruelty.
Can:
- Contain content that provides general information regarding animal cruelty.
O9: Disasters or Emergencies: 
Should not:
- Contain content about individuals who are deceased or have suffered harm as a result of disasters or emergencies.
- Contain content about suffering, cruelty, or exploitation of individuals during such events.
Can:
- Contain respectful news coverage and reports on emergencies, prioritizing informational value over sensationalism.
- Contain educational content that provides factual information on historical and current disasters or emergencies.
Assessment Steps:
1. Select a safety "rating". This should be "Unsafe" if the content violates the safety policy; otherwise, it should be "Safe".
2. Assign a policy "category". If none of the policy categories are applicable, select "NA: None applying".
3. Provide a "rationale". Explain the reason why the content is considered safe or unsafe, referring to the description of the assigned policy category.
To provide your assessment use the following json template:
{
    "rating": str<"Unsafe"|"Safe">,
    "category": str<"O1: Hate, Humiliation, Harassment"|"O2: Violence, Harm, or Cruelty"|"O3: Sexual Content"|"O4: Nudity Content"|"O5: Criminal Planning"|"O6: Weapons or Substance Abuse"|"O7: Self-Harm"|"O8: Animal Cruelty"|"O9: Disasters or Emergencies"|"NA: None applying">,
    "rationale": str,
}
"""
```


## Evaluation
We evaluate LlavaGuard on a diverse set of metrics, including balanced accuracy, Recall, and policy exception rate (PER). 
The evaluation scripts can be found in the [evaluation folder](https://github.com/ml-research/LlavaGuard/blob/main/llavaguard/sglang/evaluation/evaluation_wrapper.py).

<div align="center">
  <img src="figs/metrics.png" width="600"  alt="">
</div>
